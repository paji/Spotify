name: Voicy URL Scraper

on:
  schedule:
    - cron: '31 * * * *'  # 毎時31分に実行
  workflow_dispatch:  # 手動実行用

# 明示的に権限を設定
permissions:
  contents: write
  actions: write

env:
  PYTHONUNBUFFERED: 1  # リアルタイムでログ出力を確認できるようにする

jobs:
  voicy-url-scraper:
    runs-on: ubuntu-latest
    
    steps:
      - name: チェックアウト
        uses: actions/checkout@v4
      
      - name: Python 3.10 セットアップ
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: 必要なシステムパッケージのインストール
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb libglib2.0-0 libnss3 libgconf-2-4 libfontconfig1 libx11-xcb1 libxcb-dri3-0 libxcomposite1 libxcursor1 libxdamage1 libxfixes3 libxi6 libxrandr2 libxss1 libxtst6 fonts-liberation libasound2 libatk-bridge2.0-0 libatk1.0-0 libcups2 libdbus-1-3 libgtk-3-0
      
      - name: 最新のChromeをインストール
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          google-chrome --version
      
      - name: Chrome ブラウザのセットアップ
        uses: browser-actions/setup-chrome@latest
      
      - name: 依存関係のインストール
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 selenium webdriver-manager tqdm
      
      - name: Voicy URL スクレイピングスクリプト作成
        run: |
          cat > voicy_url_scraper.py << 'EOF'
          import os
          import re
          import json
          import time
          import random
          from datetime import datetime, timedelta
          from bs4 import BeautifulSoup
          import traceback
          from tqdm import tqdm
          from selenium import webdriver
          from selenium.webdriver.chrome.options import Options
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.common.by import By
          from selenium.webdriver.support.ui import WebDriverWait
          from selenium.webdriver.support import expected_conditions as EC
          from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
          from webdriver_manager.chrome import ChromeDriverManager

          # Voicyチャンネル情報
          CHANNEL_ID = "2834"  # 裏・パジちゃんねる
          CHANNEL_URL = f"https://voicy.jp/channel/{CHANNEL_ID}/all"  # チャンネル全エピソードページ

          # 出力ファイル設定
          OUTPUT_DIR = "output"
          OUTPUT_JSON = os.path.join(OUTPUT_DIR, "voicy_episodes.json")
          OUTPUT_URLS_ONLY = os.path.join(OUTPUT_DIR, "voicy_urls_only.json")
          DEBUG_DIR = os.path.join(OUTPUT_DIR, "debug")

          # スクレイピング設定
          MAX_RETRIES = 5  # 最大リトライ回数
          SCROLL_PAUSE_TIME = 1  # スクロール間の待機時間（秒）
          MAX_SCROLL_ATTEMPTS = 2  # 最大スクロール試行回数（約20件のエピソードを取得するため）
          TARGET_EPISODES = 20  # 目標エピソード数

          def setup_directories():
              """必要なディレクトリを作成"""
              for directory in [OUTPUT_DIR, DEBUG_DIR]:
                  os.makedirs(directory, exist_ok=True)
                  print(f"ディレクトリを確認/作成しました: {directory}")

          def random_sleep(min_seconds=0.5, max_seconds=1.5):
              """ランダムな時間スリープする（サーバー負荷軽減のため）"""
              sleep_time = random.uniform(min_seconds, max_seconds)
              time.sleep(sleep_time)
              return sleep_time

          def load_existing_episodes():
              """
              既存のJSONファイルからエピソード情報を読み込む関数
              
              Returns:
                  tuple: (既存のエピソードリスト, 既存のエピソードIDのセット)
              """
              existing_episodes = []
              existing_episode_ids = set()
              
              if os.path.exists(OUTPUT_JSON):
                  try:
                      with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:
                          existing_episodes = json.load(f)
                          
                      # エピソードIDのセットを作成
                      for episode in existing_episodes:
                          if "id" in episode:
                              existing_episode_ids.add(episode["id"])
                      
                      print(f"既存のJSONファイルから {len(existing_episodes)} 件のエピソード情報を読み込みました。")
                      print(f"重複チェック用に {len(existing_episode_ids)} 件のエピソードIDを準備しました。")
                  except Exception as e:
                      print(f"既存のJSONファイルの読み込み中にエラーが発生しました: {e}")
              else:
                  print(f"既存のJSONファイル {OUTPUT_JSON} が見つかりませんでした。新規作成します。")
              
              return existing_episodes, existing_episode_ids

          def setup_webdriver():
              """
              Seleniumのウェブドライバーをセットアップする関数
              
              Returns:
                  webdriver: 設定済みのChromeウェブドライバー
              """
              try:
                  # Chromeオプションの設定
                  chrome_options = Options()
                  
                  # ユーザーデータディレクトリに一意の値を設定
                  import uuid
                  import tempfile
                  
                  # 一時ディレクトリに一意のユーザーデータディレクトリを作成
                  unique_dir = os.path.join(tempfile.gettempdir(), f"chrome_user_data_{uuid.uuid4().hex}")
                  chrome_options.add_argument(f"--user-data-dir={unique_dir}")
                  
                  # ヘッドレスモードを無効化（検出回避のため）
                  # chrome_options.add_argument("--headless")
                  chrome_options.add_argument("--no-sandbox")
                  chrome_options.add_argument("--disable-dev-shm-usage")
                  chrome_options.add_argument("--disable-gpu")
                  chrome_options.add_argument("--window-size=1920,1080")
                  chrome_options.add_argument("--disable-extensions")
                  chrome_options.add_argument("--disable-infobars")
                  chrome_options.add_argument("--disable-notifications")
                  chrome_options.add_argument("--lang=ja")
                  # 一般的なUser-Agentを設定
                  chrome_options.add_argument('--user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"')
                  
                  # ウェブドライバーの初期化
                  service = Service()
                  driver = webdriver.Chrome(service=service, options=chrome_options)
                  
                  # タイムアウト設定
                  driver.set_page_load_timeout(60)  # 60秒のタイムアウト
                  
                  # 暗黙的な待機を設定（すべてのfind_element操作に適用）
                  driver.implicitly_wait(20)
                  
                  print("ウェブドライバーを正常にセットアップしました。")
                  return driver
              
              except Exception as e:
                  print(f"ウェブドライバーのセットアップ中にエラーが発生しました: {e}")
                  traceback.print_exc()
                  
                  # リトライを試みる
                  try:
                      print("別の設定でリトライします...")
                      chrome_options = Options()
                      chrome_options.add_argument("--no-sandbox")
                      chrome_options.add_argument("--disable-dev-shm-usage")
                      chrome_options.add_argument("--headless")  # リトライ時はヘッドレスモードを使用
                      
                      # ユーザーデータディレクトリを使用しないように設定
                      chrome_options.add_argument("--user-data-dir=")  # 空の値を設定
                      
                      # 一般的なUser-Agentを設定
                      chrome_options.add_argument('--user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"')
                      
                      service = Service()
                      driver = webdriver.Chrome(service=service, options=chrome_options)
                      
                      # タイムアウト設定
                      driver.set_page_load_timeout(60)
                      
                      # 暗黙的な待機を設定
                      driver.implicitly_wait(20)
                      
                      print("リトライ成功: ウェブドライバーをセットアップしました。")
                      return driver
                  except Exception as retry_e:
                      print(f"リトライも失敗しました: {retry_e}")
                      traceback.print_exc()
                      raise

          # メイン関数
          def main():
              """メイン関数"""
              print("Voicy URL スクレイピングを開始します。")
              
              # 必要なディレクトリをセットアップ
              setup_directories()
              
              # 既存のエピソード情報を読み込む
              existing_episodes, existing_episode_ids = load_existing_episodes()
              
              # ウェブドライバーをセットアップ
              driver = None
              try:
                  driver = setup_webdriver()
                  
                  # エピソード情報をスクレイピング
                  episodes = scrape_voicy_episodes(driver, existing_episode_ids)
                  
                  # 既存エピソードの有料/無料ステータスを更新
                  updated_existing_episodes = update_existing_episodes_paid_status(driver, existing_episodes)
                  
                  # 新規エピソードがある場合
                  if episodes:
                      print(f"{len(episodes)} 件の新規エピソードを取得しました。")
                      
                      # 一時ファイルとして保存（デバッグ用）
                      if len(episodes) > 0:
                          save_episodes_to_json(merge_episodes(updated_existing_episodes, episodes), is_temp=True)
                      
                      # 既存のエピソードと新規エピソードをマージ
                      all_episodes = merge_episodes(updated_existing_episodes, episodes)
                      
                      # JSONファイルに保存
                      save_episodes_to_json(all_episodes)
                      
                      # URLのみのJSONファイルに保存
                      save_urls_only_to_json(all_episodes)
                      
                      print(f"合計 {len(all_episodes)} 件のエピソード情報を保存しました。")
                      return True
                  else:
                      print("新規エピソードは見つかりませんでした。")
                      # 追加エピソードがない場合でも、有料/無料ステータスを更新して保存
                      save_episodes_to_json(updated_existing_episodes)
                      save_urls_only_to_json(updated_existing_episodes)
                      print(f"既存の {len(updated_existing_episodes)} 件のエピソードの有料/無料ステータスを更新しました。")
                      return False
                  
              except Exception as e:
                  print(f"エラーが発生しました: {e}")
                  traceback.print_exc()
                  return False
              finally:
                  if driver:
                      driver.quit()

          if __name__ == "__main__":
              main()
          EOF

      - name: Xvfbを使用してスクリプトを実行
        run: |
          # Xvfbをインストール（まだインストールされていない場合）
          sudo apt-get install -y xvfb
          
          # Xvfbを使用してスクリプトを実行
          xvfb-run --server-args="-screen 0 1920x1080x24" python voicy_url_scraper.py
      
      - name: 結果をコミット
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add output/
          git commit -m "Update Voicy episodes data" || echo "No changes to commit"
          git push

          def setup_directories():
              """必要なディレクトリを作成"""
              for directory in [OUTPUT_DIR, DEBUG_DIR]:
                  os.makedirs(directory, exist_ok=True)
                  print(f"ディレクトリを確認/作成しました: {directory}")

          def random_sleep(min_seconds=0.5, max_seconds=1.5):
              """ランダムな時間スリープする（サーバー負荷軽減のため）"""
              sleep_time = random.uniform(min_seconds, max_seconds)
              time.sleep(sleep_time)
              return sleep_time

          def load_existing_episodes():
              """
              既存のJSONファイルからエピソード情報を読み込む関数
              
              Returns:
                  tuple: (既存のエピソードリスト, 既存のエピソードIDのセット)
              """
              existing_episodes = []
              existing_episode_ids = set()
              
              if os.path.exists(OUTPUT_JSON):
                  try:
                      with open(OUTPUT_JSON, 'r', encoding='utf-8') as f:
                          existing_episodes = json.load(f)
                          
                      # エピソードIDのセットを作成
                      for episode in existing_episodes:
                          if "id" in episode:
                              existing_episode_ids.add(episode["id"])
                      
                      print(f"既存のJSONファイルから {len(existing_episodes)} 件のエピソード情報を読み込みました。")
                      print(f"重複チェック用に {len(existing_episode_ids)} 件のエピソードIDを準備しました。")
                  except Exception as e:
                      print(f"既存のJSONファイルの読み込み中にエラーが発生しました: {e}")
              else:
                  print(f"既存のJSONファイル {OUTPUT_JSON} が見つかりませんでした。新規作成します。")
              
              return existing_episodes, existing_episode_ids

          def parse_date_from_element(parent):
              """
              親要素からtimeタグやdata属性を使用して日付情報を取得する関数
              
              Args:
                  parent: 親要素（Seleniumの要素オブジェクト）
                  
              Returns:
                  tuple: (datetime オブジェクト, 日付文字列)
              """
              episode_date = None
              date_str = None
              
              try:
                  # 1. timeタグを優先的に探す
                  try:
                      time_element = parent.find_element(By.CSS_SELECTOR, "time")
                      
                      # data-timestamp属性を確認（Unixタイムスタンプ）
                      timestamp = time_element.get_attribute("data-timestamp")
                      if timestamp and timestamp.isdigit():
                          # Unixタイムスタンプを日時に変換
                          episode_date = datetime.fromtimestamp(int(timestamp))
                          date_str = time_element.text.strip()
                          return episode_date, date_str
                      
                      # datetime属性を確認（ISO形式の日時）
                      datetime_attr = time_element.get_attribute("datetime")
                      if datetime_attr:
                          try:
                              # ISO形式の日時を解析
                              episode_date = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                              date_str = time_element.text.strip()
                              return episode_date, date_str
                          except ValueError:
                              pass
                      
                      # timeタグのテキストを取得
                      date_str = time_element.text.strip()
                  except:
                      # timeタグが見つからない場合は他の日付要素を探す
                      try:
                          date_element = parent.find_element(By.CSS_SELECTOR, ".date, .episode-date")
                          date_str = date_element.text.strip()
                      except:
                          date_str = None
              except:
                  date_str = None
              
              # 日付文字列が取得できた場合は解析を試みる
              if date_str:
                  episode_date = parse_date_string(date_str)
              
              return episode_date, date_str

          def parse_date_string(date_str):
              """
              日付文字列を解析してdatetimeオブジェクトに変換する関数
              
              Args:
                  date_str: 日付を表す文字列
                  
              Returns:
                  datetime: 解析された日時オブジェクト、解析失敗時はNone
              """
              if not date_str:
                  return None
              
              # 現在の日時を取得（相対日付計算用）
              now = datetime.now()
              
              try:
                  # 1. 相対日付の解析（例: 3日前、1週間前、2時間前）
                  relative_match = re.search(r'(\d+)\s*(時間|日|週間|ヶ月|カ月|ヵ月|か月|年)\s*前', date_str)
                  if relative_match:
                      value = int(relative_match.group(1))
                      unit = relative_match.group(2)
                      
                      if unit == '時間':
                          return now - timedelta(hours=value)
                      elif unit == '日':
                          return now - timedelta(days=value)
                      elif unit == '週間':
                          return now - timedelta(weeks=value)
                      elif unit in ['ヶ月', 'カ月', 'ヵ月', 'か月']:
                          # 月は正確な日数がないので近似値を使用
                          return now - timedelta(days=value * 30)
                      elif unit == '年':
                          # 年も近似値を使用
                          return now - timedelta(days=value * 365)
                  
                  # 2. 「今日」「昨日」「一昨日」などの特殊な相対日付
                  if '今日' in date_str:
                      return now
                  elif '昨日' in date_str:
                      return now - timedelta(days=1)
                  elif '一昨日' in date_str or '2日前' in date_str:
                      return now - timedelta(days=2)
                  
                  # 3. 日付フォーマットのパターンを試行
                  date_formats = [
                      # 年月日 時分 形式
                      "%Y年%m月%d日 %H時%M分",
                      "%Y年%m月%d日 %H:%M",
                      "%Y/%m/%d %H:%M",
                      "%Y-%m-%d %H:%M",
                      # 年月日のみ
                      "%Y年%m月%d日",
                      "%Y/%m/%d",
                      "%Y-%m-%d",
                      # 月日 時分 形式
                      "%m月%d日 %H時%M分",
                      "%m月%d日 %H:%M",
                      "%m/%d %H:%M",
                      # 月日のみ
                      "%m月%d日",
                      "%m/%d"
                  ]
                  
                  for date_format in date_formats:
                      try:
                          # 時間情報を含まないフォーマットで年が省略されている場合
                          if (len(date_format.split()) > 0 and "年" not in date_format.split()[0] and "/" not in date_format.split()[0] and "-" not in date_format.split()[0]) and \
                             (len(date_str.split()) > 0 and "年" not in date_str.split()[0] and "/" not in date_str.split()[0] and "-" not in date_str.split()[0]):
                              # 現在の年を使用
                              parsed_date = datetime.strptime(date_str, date_format)
                              return parsed_date.replace(year=now.year)
                          
                          # 完全な日付
                          return datetime.strptime(date_str, date_format)
                      except (ValueError, IndexError):
                          continue
              
              except Exception as e:
                  print(f"日付の解析中にエラーが発生しました: {e}")
              
              return None

          def setup_webdriver():
              """
              Seleniumのウェブドライバーをセットアップする関数
              
              Returns:
                  webdriver: 設定済みのChromeウェブドライバー
              """
              try:
                  # Chromeオプションの設定
                  chrome_options = Options()
                  
                  # ユーザーデータディレクトリに一意の値を設定
                  import uuid
                  import tempfile
                  
                  # 一時ディレクトリに一意のユーザーデータディレクトリを作成
                  unique_dir = os.path.join(tempfile.gettempdir(), f"chrome_user_data_{uuid.uuid4().hex}")
                  chrome_options.add_argument(f"--user-data-dir={unique_dir}")
                  
                  # ヘッドレスモードを無効化（検出回避のため）
                  # chrome_options.add_argument("--headless")
                  chrome_options.add_argument("--no-sandbox")
                  chrome_options.add_argument("--disable-dev-shm-usage")
                  chrome_options.add_argument("--disable-gpu")
                  chrome_options.add_argument("--window-size=1920,1080")
                  chrome_options.add_argument("--disable-extensions")
                  chrome_options.add_argument("--disable-infobars")
                  chrome_options.add_argument("--disable-notifications")
                  chrome_options.add_argument("--lang=ja")
                  # 一般的なUser-Agentを設定
                  chrome_options.add_argument('--user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"')
                  
                  # ウェブドライバーの初期化
                  service = Service()
                  driver = webdriver.Chrome(service=service, options=chrome_options)
                  
                  # タイムアウト設定
                  driver.set_page_load_timeout(60)  # 60秒のタイムアウト
                  
                  # 暗黙的な待機を設定（すべてのfind_element操作に適用）
                  driver.implicitly_wait(20)
                  
                  print("ウェブドライバーを正常にセットアップしました。")
                  return driver
              
              except Exception as e:
                  print(f"ウェブドライバーのセットアップ中にエラーが発生しました: {e}")
                  traceback.print_exc()
                  
                  # リトライを試みる
                  try:
                      print("別の設定でリトライします...")
                      chrome_options = Options()
                      chrome_options.add_argument("--no-sandbox")
                      chrome_options.add_argument("--disable-dev-shm-usage")
                      chrome_options.add_argument("--headless")  # リトライ時はヘッドレスモードを使用
                      
                      # ユーザーデータディレクトリを使用しないように設定
                      chrome_options.add_argument("--user-data-dir=")  # 空の値を設定
                      
                      # 一般的なUser-Agentを設定
                      chrome_options.add_argument('--user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"')
                      
                      service = Service()
                      driver = webdriver.Chrome(service=service, options=chrome_options)
                      
                      # タイムアウト設定
                      driver.set_page_load_timeout(60)
                      
                      # 暗黙的な待機を設定
                      driver.implicitly_wait(20)
                      
                      print("リトライ成功: ウェブドライバーをセットアップしました。")
                      return driver
                  except Exception as retry_e:
                      print(f"リトライも失敗しました: {retry_e}")
                      traceback.print_exc()
                      raise

          def scrape_voicy_episodes(driver, existing_episode_ids):
              """
              Voicyのエピソード情報をスクレイピングする関数
              
              Args:
                  driver: Seleniumのウェブドライバー
                  existing_episode_ids: 既存のエピソードIDのセット（重複チェック用）
                  
              Returns:
                  list: 新規エピソード情報のリスト
              """
              episodes = []
              new_episode_count = 0
              
              try:
                  print(f"Voicyチャンネルページにアクセスします: {CHANNEL_URL}")
                  driver.get(CHANNEL_URL)
                  
                  # ページの読み込みを待機
                  try:
                      # ページが完全に読み込まれるまで待機
                      WebDriverWait(driver, 30).until(
                          lambda d: d.execute_script("return document.readyState") == "complete"
                      )
                      
                      # エピソードリスト要素が表示されるまで待機
                      WebDriverWait(driver, 30).until(
                          EC.presence_of_element_located((By.CSS_SELECTOR, ".episode-list, .episodes-container"))
                      )
                      
                      print("ページの読み込みが完了しました。エピソードの取得を開始します。")
                  except TimeoutException:
                      print("ページの読み込みがタイムアウトしましたが、処理を続行します。")
                      # 追加の待機時間を設定
                      time.sleep(5)
                  
                  # スクロール処理を改善
                  print("スクロール処理を開始します...")
                  
                  # 初期HTMLをデバッグ用に保存
                  with open(os.path.join(DEBUG_DIR, "initial_page.html"), "w", encoding="utf-8") as f:
                      f.write(driver.page_source)
                  
                  # スクロール回数のカウンター
                  scroll_count = 0
                  total_scrolls = 10  # より多くのスクロール回数
                  
                  # 目標エピソード数に達するか、最大スクロール回数に達するまでスクロールを繰り返す
                  while scroll_count < total_scrolls:
                      try:
                          # スクロール位置を計算（徐々に下へ）
                          scroll_height = driver.execute_script("return document.body.scrollHeight")
                          scroll_position = (scroll_count + 1) * scroll_height / total_scrolls
                          
                          # スクロール実行
                          driver.execute_script(f"window.scrollTo(0, {scroll_position});")
                          
                          # スクロール後の待機
                          time.sleep(SCROLL_PAUSE_TIME * 2)  # 待機時間を2倍に
                          
                          # 途中でスクリーンショットを取得（デバッグ用）
                          if scroll_count % 3 == 0:  # 3回ごとにスクリーンショットを保存
                              try:
                                  driver.save_screenshot(os.path.join(DEBUG_DIR, f"scroll_{scroll_count}.png"))
                              except Exception as e:
                                  print(f"スクリーンショット保存中にエラーが発生しました: {e}")
                          
                          scroll_count += 1
                          print(f"スクロール {scroll_count}/{total_scrolls} 完了")
                          
                          # 最後のスクロール後は少し長めに待機
                          if scroll_count == total_scrolls:
                              print("最終スクロール後の待機中...")
                              time.sleep(3)
                      except Exception as e:
                          print(f"スクロール中にエラーが発生しました: {e}")
                          break
                  
                  # 現在のHTMLをデバッグ用に保存
                  with open(os.path.join(DEBUG_DIR, "after_scroll.html"), "w", encoding="utf-8") as f:
                      f.write(driver.page_source)
                  
                  # エピソード要素を取得（複数のセレクタを試行）
                  episode_elements = []
                  
                  # セレクタ候補（Voicyのサイト構造が変わっている可能性があるため複数試す）
                  selectors = [
                      ".episode-item, .episode-card",  # 元のセレクタ
                      "div[class*='episodeItem']",  # クラス名に'episodeItem'を含む要素
                      "div.episodeItem",  # 直接クラスで検索
                      "div[class*='episode']",  # クラス名に'episode'を含む要素
                      "div.episode",  # 直接クラスで検索
                      "a[href*='/episode/']",  # エピソードへのリンク
                      "div.sc-iNGGcK",  # 可能性のあるクラス
                      "div[class*='epfKLr']",  # 可能性のあるクラス
                      "div.epfKLr",  # 可能性のあるクラス
                      "div[class*='card']",  # カードスタイルの要素
                      "div.card"  # カードスタイルの要素
                  ]
                  
                  # 各セレクタで試行
                  for selector in selectors:
                      try:
                          elements = driver.find_elements(By.CSS_SELECTOR, selector)
                          if elements:
                              episode_elements = elements
                              print(f"セレクタ '{selector}' で {len(elements)} 件のエピソード要素を検出しました。")
                              break
                      except Exception as e:
                          print(f"セレクタ '{selector}' の検索中にエラーが発生しました: {str(e)}")
                  
                  # エピソード要素が見つからない場合のフォールバック処理
                  if not episode_elements:
                      print("通常のセレクタでエピソード要素が見つかりませんでした。フォールバック方法を試行します。")
                      
                      try:
                          # すべてのリンクを取得してエピソードURLを抽出
                          all_links = driver.find_elements(By.TAG_NAME, "a")
                          episode_links = []
                          
                          for link in all_links:
                              try:
                                  href = link.get_attribute("href")
                                  if href and ("/episode/" in href or f"/channel/{CHANNEL_ID}/" in href):
                                      episode_links.append(link)
                              except:
                                  continue
                          
                          print(f"{len(episode_links)} 件のエピソードらしきリンクを検出しました。")
                          
                          # リンクからエピソード情報を直接抽出
                          for link in episode_links:
                              try:
                                  href = link.get_attribute("href")
                                  if not href:
                                      continue
                                      
                                  # エピソードIDを抽出
                                  if "/episode/" in href:
                                      episode_id = href.split("/episode/")[1].split("/")[0]
                                  else:
                                      episode_id = href.split(f"/channel/{CHANNEL_ID}/")[1].split("/")[0]
                                  
                                  # 既存のエピソードIDと重複チェック
                                  if episode_id in existing_episode_ids:
                                      continue
                                  
                                  # タイトルを取得（可能であれば）
                                  title = "タイトル不明"
                                  try:
                                      # リンク内のテキストを取得
                                      title_text = link.text.strip()
                                      if title_text:
                                          title = title_text
                                      else:
                                          # 親要素内のテキストを取得
                                          parent = link.find_element(By.XPATH, "./..")
                                          title = parent.text.strip()
                                  except:
                                      pass
                                  
                                  # 日付情報（現在時刻をデフォルトとして使用）
                                  date_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                  
                                  # エピソード情報を辞書として保存
                                  episode_info = {
                                      "id": episode_id,
                                      "title": title,
                                      "date": date_str,
                                      "url": href,
                                      "is_paid": False  # デフォルトは無料と仮定
                                  }
                                  
                                  episodes.append(episode_info)
                                  new_episode_count += 1
                              except Exception as e:
                                  print(f"リンクからのエピソード情報抽出中にエラーが発生しました: {e}")
                                  continue
                      except Exception as e:
                          print(f"フォールバック処理中にエラーが発生しました: {e}")
                  
                  # 通常のエピソード要素処理
                  if episode_elements:
                      print(f"{len(episode_elements)} 件のエピソード要素を検出しました。")
                  
                  # 各エピソード要素から情報を抽出
                  for element in tqdm(episode_elements, desc="エピソード情報の抽出"):
                      try:
                          # エピソードIDを取得
                          episode_url = element.find_element(By.CSS_SELECTOR, "a").get_attribute("href")
                          episode_id = episode_url.split("/")[-1]
                          
                          # 既存のエピソードIDと重複チェック
                          if episode_id in existing_episode_ids:
                              continue
                          
                          # エピソードタイトルを取得
                          title_element = element.find_element(By.CSS_SELECTOR, ".episode-title, .title")
                          episode_title = title_element.text.strip()
                          
                          # 日付情報を取得
                          episode_date, date_str = parse_date_from_element(element)
                          
                          # 日付が取得できなかった場合は現在時刻を使用
                          if not episode_date:
                              episode_date = datetime.now()
                              date_str = episode_date.strftime("%Y-%m-%d %H:%M:%S")
                          else:
                              # 日付文字列のフォーマットを統一
                              date_str = episode_date.strftime("%Y-%m-%d %H:%M:%S")
                          
                          # 有料配信かどうかを判定
                          is_paid = False
                          try:
                              # 方法1: 「P」アイコンを探す（黄色い円形のアイコン）
                              paid_icon = element.find_elements(By.CSS_SELECTOR, ".paid-icon, img[src*='paid'], div[class*='paid'], .premium-icon")
                              if paid_icon:
                                  is_paid = True
                              
                              # 方法2: 「プレミアムリスナーに参加」ボタンを探す（有料配信の特徴）
                              premium_buttons = element.find_elements(By.XPATH, ".//button[contains(text(), 'プレミアムリスナーに参加')] | .//a[contains(text(), 'プレミアムリスナーに参加')]")
                              if premium_buttons:
                                  is_paid = True
                              
                              # 方法3: 「再生する」ボタンを探す（無料配信の特徴）
                              play_buttons = element.find_elements(By.XPATH, ".//button[contains(text(), '再生する')] | .//a[contains(text(), '再生する')]")
                              if play_buttons:
                                  is_paid = False
                              
                              # 方法4: 画像の「P」アイコンを探す
                              img_elements = element.find_elements(By.TAG_NAME, "img")
                              for img in img_elements:
                                  src = img.get_attribute("src")
                                  alt = img.get_attribute("alt")
                                  if (src and ("paid" in src.lower() or "premium" in src.lower())) or \
                                     (alt and ("paid" in alt.lower() or "premium" in alt.lower() or "p" == alt.lower())):
                                      is_paid = True
                                      break
                                      
                              # 方法5: プレミアムに関連するテキストを探す
                              premium_text = element.find_elements(By.XPATH, ".//*[contains(text(), 'プレミアム') or contains(text(), '有料')]")
                              if premium_text:
                                  is_paid = True
                          except:
                              # エラーが発生した場合は無料と判定
                              is_paid = False
                          
                          # エピソード情報を辞書として保存
                          episode_info = {
                              "id": episode_id,
                              "title": episode_title,
                              "date": date_str,
                              "url": episode_url,
                              "is_paid": is_paid
                          }
                          
                          episodes.append(episode_info)
                          new_episode_count += 1
                          
                      except Exception as e:
                          print(f"エピソード情報の抽出中にエラーが発生しました: {e}")
                          continue
                  
                  print(f"スクレイピングが完了しました。{new_episode_count} 件の新規エピソードを取得しました。")
                  
              except Exception as e:
                  print(f"スクレイピング中にエラーが発生しました: {e}")
                  traceback.print_exc()
              
              return episodes

          def update_existing_episodes_paid_status(driver, existing_episodes):
              """
              既存のエピソードの有料/無料ステータスを更新する関数
              
              Args:
                  driver: Seleniumのウェブドライバー
                  existing_episodes: 既存のエピソードリスト
                  
              Returns:
                  list: 有料/無料ステータスが更新されたエピソードリスト
              """
              print("既存エピソードの有料/無料ステータスを更新しています...")
              updated_episodes = []
              
              for episode in tqdm(existing_episodes, desc="既存エピソードの有料/無料ステータス更新"):
                  try:
                      # エピソードURLにアクセス
                      driver.get(episode["url"])
                      
                      # ページの読み込みを待機
                      try:
                          # ページが完全に読み込まれるまで待機
                          WebDriverWait(driver, 30).until(
                              lambda d: d.execute_script("return document.readyState") == "complete"
                          )
                          
                          # コンテンツが表示されるまで待機
                          WebDriverWait(driver, 30).until(
                              EC.presence_of_element_located((By.CSS_SELECTOR, ".episode-content, .episode-detail, .episode-info"))
                          )
                      except TimeoutException:
                          print(f"エピソード {episode.get('id', 'unknown')} のページ読み込みがタイムアウトしましたが、処理を続行します。")
                          # 追加の待機時間を設定
                          time.sleep(5)
                      
                      # 有料配信かどうかを判定
                      is_paid = False
                      
                      try:
                          # 方法1: 「プレミアムリスナーに参加」ボタンを探す（有料配信の特徴）
                          premium_buttons = driver.find_elements(By.XPATH, "//button[contains(text(), 'プレミアムリスナーに参加')] | //a[contains(text(), 'プレミアムリスナーに参加')]")
                          if premium_buttons:
                              is_paid = True
                          
                          # 方法2: 「再生する」ボタンを探す（無料配信の特徴）
                          play_buttons = driver.find_elements(By.XPATH, "//button[contains(text(), '再生する')] | //a[contains(text(), '再生する')]")
                          if play_buttons:
                              is_paid = False
                          
                          # 方法3: 「P」アイコンを探す
                          paid_icons = driver.find_elements(By.CSS_SELECTOR, ".paid-icon, img[src*='paid'], div[class*='paid'], .premium-icon")
                          if paid_icons:
                              is_paid = True
                              
                          # 方法4: プレミアムに関連するテキストを探す
                          premium_text = driver.find_elements(By.XPATH, "//*[contains(text(), 'プレミアム') or contains(text(), '有料')]")
                          if premium_text:
                              is_paid = True
                      except:
                          # エラーが発生した場合は既存の値を維持するか、デフォルトで無料と判定
                          is_paid = episode.get("is_paid", False)
                      
                      # エピソード情報を更新
                      updated_episode = episode.copy()
                      updated_episode["is_paid"] = is_paid
                      updated_episodes.append(updated_episode)
                      
                  except Exception as e:
                      print(f"エピソード {episode.get('id', 'unknown')} の有料/無料ステータス更新中にエラーが発生しました: {e}")
                      # エラーが発生した場合は元のエピソード情報を維持
                      updated_episodes.append(episode)
              
              print(f"{len(updated_episodes)} 件のエピソードの有料/無料ステータスを更新しました。")
              return updated_episodes

          def merge_episodes(existing_episodes, new_episodes):
              """
              既存のエピソードと新規エピソードをマージする関数
              
              Args:
                  existing_episodes: 既存のエピソードリスト
                  new_episodes: 新規エピソードリスト
                  
              Returns:
                  list: マージされたエピソードリスト
              """
              # 既存のエピソードIDのセットを作成
              existing_ids = {episode["id"] for episode in existing_episodes}
              
              # 重複しないエピソードのみを追加
              merged_episodes = existing_episodes.copy()
              
              for episode in new_episodes:
                  if episode["id"] not in existing_ids:
                      merged_episodes.append(episode)
                      existing_ids.add(episode["id"])
              
              # マージされたエピソードリストを返す
              return merged_episodes
              
          def save_episodes_to_json(episodes, is_temp=False):
              """
              エピソード情報をJSONファイルに保存する関数
              
              Args:
                  episodes: エピソード情報のリスト
                  is_temp: 一時ファイルとして保存するかどうか
              """
              try:
                  output_path = OUTPUT_JSON
                  if is_temp:
                      output_path = os.path.join(OUTPUT_DIR, f"voicy_episodes_temp_{len(episodes)}.json")
                  
                  # 配信日時でエピソードをソート（最新のエピソードが上部に来るように降順ソート）
                  sorted_episodes = sorted(
                      episodes,
                      key=lambda x: datetime.strptime(x.get("date", "1970-01-01 00:00:00"), "%Y-%m-%d %H:%M:%S"),
                      reverse=True
                  )
                  
                  with open(output_path, 'w', encoding='utf-8') as f:
                      json.dump(sorted_episodes, f, ensure_ascii=False, indent=2)
                  
                  if is_temp:
                      print(f"一時エピソード情報をJSONファイルに保存しました: {output_path}")
                  else:
                      print(f"エピソード情報をJSONファイルに保存しました: {output_path}")
              except Exception as e:
                  print(f"JSONファイルの保存中にエラーが発生しました: {e}")

          def save_urls_only_to_json(episodes):
              """
              URLのみのリストをJSONファイルに保存する関数
              
              Args:
                  episodes: エピソード情報のリスト
              """
              try:
                  # URLのみのリストを作成
                  urls = [episode["url"] for episode in episodes]
                  
                  with open(OUTPUT_URLS_ONLY, 'w', encoding='utf-8') as f:
                      json.dump(urls, f, ensure_ascii=False, indent=2)
                  
                  print(f"URLのみのリストをJSONファイルに保存しました: {OUTPUT_URLS_ONLY}")
              except Exception as e:
                  print(f"URLのみのJSONファイルの保存中にエラーが発生しました: {e}")

          def main():
              """メイン関数"""
              print("Voicy URL スクレイピングを開始します。")
              
              # 必要なディレクトリをセットアップ
              setup_directories()
              
              # 既存のエピソード情報を読み込む
              existing_episodes, existing_episode_ids = load_existing_episodes()
              
              # ウェブドライバーをセットアップ
              driver = None
              try:
                  driver = setup_webdriver()
                  
                  # エピソード情報をスクレイピング
                  episodes = scrape_voicy_episodes(driver, existing_episode_ids)
                  
                  # 既存エピソードの有料/無料ステータスを更新
                  updated_existing_episodes = update_existing_episodes_paid_status(driver, existing_episodes)
                  
                  # 新規エピソードがある場合
                  if episodes:
                      print(f"{len(episodes)} 件の新規エピソードを取得しました。")
                      
                      # 一時ファイルとして保存（デバッグ用）
                      if len(episodes) > 0:
                          save_episodes_to_json(merge_episodes(updated_existing_episodes, episodes), is_temp=True)
                      
                      # 既存のエピソードと新規エピソードをマージ
                      all_episodes = merge_episodes(updated_existing_episodes, episodes)
                      
                      # JSONファイルに保存
                      save_episodes_to_json(all_episodes)
                      
                      # URLのみのJSONファイルに保存
                      save_urls_only_to_json(all_episodes)
                      
                      print(f"合計 {len(all_episodes)} 件のエピソード情報を保存しました。")
                      return True
                  else:
                      print("新規エピソードは見つかりませんでした。")
                      # 追加エピソードがない場合でも、有料/無料ステータスを更新して保存
                      save_episodes_to_json(updated_existing_episodes)
                      save_urls_only_to_json(updated_existing_episodes)
                      print(f"既存の {len(updated_existing_episodes)} 件のエピソードの有料/無料ステータスを更新しました。")
                      return False
                  
              except Exception as e:
                  print(f"エラーが発生しました: {e}")
                  traceback.print_exc()
                  return False
              
              finally:
                  # ウェブドライバーを終了
                  if driver:
                      driver.quit()
                      print("ウェブドライバーを終了しました。")

          if __name__ == "__main__":
              main()
          EOF
      
      - name: Voicy URL スクレイピングを実行
        run: python voicy_url_scraper.py
      
      - name: 変更をコミット
        run: |
          git config --local user.email "actions@github.com"
          git config --local user.name "actions-user"
          git add output/voicy_episodes.json output/voicy_urls_only.json
          git diff --staged --quiet || git commit -m "Update Voicy episodes JSON"
          git push
